{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tqdm\n",
    "import os\n",
    "os.chdir('/vol/medic01/users/bh1511/PyCharm_Deployment/ResNetAE')\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ResNetAE import ResNetVQVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### ChestXray14 ######################################################################################################\n",
    "\n",
    "# data_root = '/vol/biomedic/users/bh1511/datasets/cxr/ChestXray-NIHCC'\n",
    "data_root = '/data/datasets/chest_xray/ChestXray-NIHCC/'\n",
    "\n",
    "train_val_df = pd.read_csv(os.path.join(data_root, 'train_val_list.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_root, 'test_list.csv'))\n",
    "\n",
    "train_images_df = data_root + '/images/' + train_val_df['Image Index']\n",
    "val_images_df = data_root + '/images/' + test_df['Image Index']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### HParams ##########################################################################################################\n",
    "\n",
    "NUM_LATENT_K = 512                # Number of codebook entries\n",
    "NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "BETA = 0.25                       # Weight for the commitment loss\n",
    "\n",
    "INPUT_SHAPE = (256, 256, 1)\n",
    "SIZE = 16                         # Spatial size of latent embedding\n",
    "\n",
    "VQVAE_BATCH_SIZE = 16             # Batch size for training the VQVAE\n",
    "VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "VQVAE_LEARNING_RATE = 1e-4        # Learning rate\n",
    "\n",
    "SAVE_DIR = 'checkpoint4'\n",
    "\n",
    "PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "PIXELCNN_NUM_EPOCHS = 100         # Number of epochs\n",
    "PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block\n",
    "\n",
    "LOAD_MODEL = False\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Tensorflow Dataset ###############################################################################################\n",
    "\n",
    "def parse_function(filename):\n",
    "    # Read entire contents of image\n",
    "    image_string = tf.io.read_file(filename)\n",
    "\n",
    "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tf.io.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    # This will convert to float values in [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    # Resize image with padding to 244x244\n",
    "    image = tf.image.resize_with_pad(image, 256, 256, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images_df))\n",
    "train_ds = train_ds.shuffle(len(train_ds))\n",
    "train_ds = train_ds.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.batch(VQVAE_BATCH_SIZE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((val_images_df))\n",
    "test_ds = test_ds.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.batch(VQVAE_BATCH_SIZE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Define Model, Optimizer and Loss #################################################################################\n",
    "\n",
    "model = ResNetVQVAE(input_shape=INPUT_SHAPE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=VQVAE_LEARNING_RATE)\n",
    "mse_loss = tf.keras.losses.MSE\n",
    "\n",
    "train_total_loss = tf.keras.metrics.Mean()\n",
    "train_VecQuant_loss = tf.keras.metrics.Mean()\n",
    "train_reconstruction_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "val_total_loss = tf.keras.metrics.Mean()\n",
    "val_VecQuant_loss = tf.keras.metrics.Mean()\n",
    "val_reconstruction_loss = tf.keras.metrics.Mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### ResNetVQVAE Training Loop ########################################################################################\n",
    "\n",
    "for epoch in range(VQVAE_NUM_EPOCHS):\n",
    "\n",
    "    train_total_loss.reset_states()\n",
    "    train_reconstruction_loss.reset_states()\n",
    "    train_VecQuant_loss.reset_states()\n",
    "\n",
    "    val_total_loss.reset_states()\n",
    "    val_reconstruction_loss.reset_states()\n",
    "    val_VecQuant_loss.reset_states()\n",
    "\n",
    "    print('=' * 50, f'Training EPOCH {epoch}', '=' * 50)\n",
    "    start = time.time()\n",
    "\n",
    "    ## Train Step\n",
    "    t = tqdm.tqdm(enumerate(train_ds), total=len(train_ds))\n",
    "    for step, data in t:\n",
    "        with tf.GradientTape() as tape:\n",
    "            vq_loss, data_recon, perplexity = model(data)\n",
    "            recon_err = mse_loss(data_recon, data)\n",
    "            loss = vq_loss + recon_err\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_total_loss(loss)\n",
    "        train_reconstruction_loss(recon_err)\n",
    "        train_VecQuant_loss(vq_loss)\n",
    "\n",
    "        t.set_description('>%d, t_loss=%.5f, recon_loss=%.5f, VecQuant_loss=%.5f' % (step,\n",
    "                                                                                     train_total_loss.result(),\n",
    "                                                                                     train_reconstruction_loss.result(),\n",
    "                                                                                     train_VecQuant_loss.result()))\n",
    "\n",
    "    ## Evaluation Step\n",
    "    for step, data in tqdm.tqdm(enumerate(test_ds), total=len(test_ds)):\n",
    "        vq_loss, data_recon, perplexity = model(data)\n",
    "        recon_err = mse_loss(data_recon, data)\n",
    "        loss = vq_loss + recon_err\n",
    "\n",
    "        val_total_loss(loss)\n",
    "        val_reconstruction_loss(recon_err)\n",
    "        val_VecQuant_loss(vq_loss)\n",
    "\n",
    "    print(f'Epoch {epoch} complete in: {time.time() - start:.5f}')\n",
    "    print('t_loss={:.5f}, recon_loss={:.5f}, VecQuant_loss={:.5f}'.format(val_total_loss.result(),\n",
    "                                                                          val_reconstruction_loss.result(),\n",
    "                                                                          val_VecQuant_loss.result()))\n",
    "\n",
    "    model.save_weights(os.path.join(SAVE_DIR, f'model_{epoch}.h5'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### ResNetVQVAE Evaluation Loop ######################################################################################\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    a=model(np.zeros((10,256,256,1)).astype('float32'))\n",
    "    model.load_weights('checkpoint4/model_42.h5')\n",
    "\n",
    "## Inference\n",
    "n_inference = 100\n",
    "input_samples = []\n",
    "output_samples = []\n",
    "print('Running Inference')\n",
    "for i, data in tqdm.tqdm(enumerate(test_ds), total=n_inference):\n",
    "    _, gen_sample, _, _ = model(data, training=False)\n",
    "    gen_sample = gen_sample.numpy()\n",
    "    input_samples.append(data)\n",
    "    output_samples.append(gen_sample)\n",
    "    if i == n_inference: break\n",
    "\n",
    "input_samples = np.concatenate(input_samples, axis=0)\n",
    "output_samples = np.concatenate(output_samples, axis=0)\n",
    "\n",
    "# assert os.path.exists(SAVE_DIR), \"Directory does not exist\"\n",
    "# print('Saving Images')\n",
    "# for i in tqdm.tqdm(range(n_inference)):\n",
    "#     cv2.imwrite(os.path.join(SAVE_DIR, f'sample_{i}_real.jpg'), input_samples[i]*255.)\n",
    "#     cv2.imwrite(os.path.join(SAVE_DIR, f'sample_{i}_gen.jpg'), output_samples[i]*255.)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 20))\n",
    "\n",
    "for i in tqdm.tqdm(range(12)):\n",
    "    ax = fig.add_subplot(4, 6, i*2 + 1)\n",
    "    ax.set_title('Original')\n",
    "    ax.imshow(np.squeeze(input_samples[i]*255.), cmap='gray')\n",
    "    ax = fig.add_subplot(4, 6, i*2 + 2)\n",
    "    ax.set_title('Reconstruction')\n",
    "    ax.imshow(np.squeeze(output_samples[i]*255.), cmap='gray')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Quantised Encodings ##############################################################################################\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    a=model(np.zeros((10, 256, 256, 1)).astype('float32'))\n",
    "    model.load_weights('checkpoint4/model_42.h5')\n",
    "\n",
    "z_train = []\n",
    "for i, data in enumerate(tqdm.tqdm(train_ds)):\n",
    "    encodings = model.vq_vae(model.pre_vq_conv(model.encoder(data)))[3]\n",
    "    z_train.append(encodings)\n",
    "    # if i == 100: break\n",
    "z_train = np.concatenate(z_train, axis=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Visualise Codes ##############################################################################################\n",
    "\n",
    "fig = plt.figure(figsize=(40, 20))\n",
    "\n",
    "for i in tqdm.tqdm(range(32)):\n",
    "    ax = fig.add_subplot(4, 8, i + 1)\n",
    "    img = ax.imshow(np.squeeze(z_train[i,...]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### PixelCNN #########################################################################################################\n",
    "\n",
    "# References:\n",
    "# https://www.kaggle.com/ameroyer/keras-vq-vae-for-image-generation\n",
    "# https://github.com/anantzoid/Conditional-PixelCNN-decoder/blob/master/layers.py\n",
    "# https://github.com/ritheshkumar95/pytorch-vqvae\n",
    "\n",
    "def gate(inputs):\n",
    "    \"\"\"Gated activations\"\"\"\n",
    "    x, y = tf.split(inputs, 2, axis=-1)\n",
    "    return tf.tanh(x) * tf.sigmoid(y)\n",
    "\n",
    "\n",
    "class MaskedConv2D(tf.keras.layers.Layer):\n",
    "    \"\"\"Masked convolution\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, out_dim, direction, mode, **kwargs):\n",
    "        self.direction = direction  # Horizontal or vertical\n",
    "        self.mode = mode  # Mask type \"a\" or \"b\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_dim = out_dim\n",
    "        super(MaskedConv2D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        filter_mid_y = self.kernel_size[0] // 2\n",
    "        filter_mid_x = self.kernel_size[1] // 2\n",
    "        in_dim = int(input_shape[-1])\n",
    "        w_shape = [self.kernel_size[0], self.kernel_size[1], in_dim, self.out_dim]\n",
    "        mask_filter = np.ones(w_shape, dtype=np.float32)\n",
    "        # Build the mask\n",
    "        if self.direction == \"h\":\n",
    "            mask_filter[filter_mid_y + 1:, :, :, :] = 0.\n",
    "            mask_filter[filter_mid_y, filter_mid_x + 1:, :, :] = 0.\n",
    "        elif self.direction == \"v\":\n",
    "            if self.mode == 'a':\n",
    "                mask_filter[filter_mid_y:, :, :, :] = 0.\n",
    "            elif self.mode == 'b':\n",
    "                mask_filter[filter_mid_y + 1:, :, :, :] = 0.0\n",
    "        if self.mode == 'a':\n",
    "            mask_filter[filter_mid_y, filter_mid_x, :, :] = 0.0\n",
    "        # Create convolution layer parameters with masked kernel\n",
    "        self.W = mask_filter * self.add_weight(\"W_{}\".format(self.direction), w_shape, trainable=True)\n",
    "        self.b = self.add_weight(\"v_b\", [self.out_dim, ], trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.keras.backend.conv2d(inputs, self.W, strides=(1, 1)) + self.b\n",
    "\n",
    "\n",
    "def gated_masked_conv2d(v_stack_in, h_stack_in, out_dim, kernel, mask='b', residual=True, i=0):\n",
    "    \"\"\"Basic Gated-PixelCNN block.\n",
    "       This is an improvement over PixelRNN to avoid \"blind spots\", i.e. pixels missingt from the\n",
    "       field of view. It works by having two parallel stacks, for the vertical and horizontal direction,\n",
    "       each being masked  to only see the appropriate context pixels.\n",
    "    \"\"\"\n",
    "    kernel_size = (kernel // 2 + 1, kernel)\n",
    "    padding = (kernel // 2, kernel // 2)\n",
    "\n",
    "    v_stack = tf.keras.layers.ZeroPadding2D(padding=padding, name=\"v_pad_{}\".format(i))(v_stack_in)\n",
    "    v_stack = MaskedConv2D(kernel_size, out_dim * 2, \"v\", mask, name=\"v_masked_conv_{}\".format(i))(v_stack)\n",
    "    v_stack = v_stack[:, :int(v_stack_in.get_shape()[-3]), :, :]\n",
    "    v_stack_out = tf.keras.layers.Lambda(lambda inputs: gate(inputs), name=\"v_gate_{}\".format(i))(v_stack)\n",
    "\n",
    "    kernel_size = (1, kernel // 2 + 1)\n",
    "    padding = (0, kernel // 2)\n",
    "    h_stack = tf.keras.layers.ZeroPadding2D(padding=padding, name=\"h_pad_{}\".format(i))(h_stack_in)\n",
    "    h_stack = MaskedConv2D(kernel_size, out_dim * 2, \"h\", mask, name=\"h_masked_conv_{}\".format(i))(h_stack)\n",
    "    h_stack = h_stack[:, :, :int(h_stack_in.get_shape()[-2]), :]\n",
    "    h_stack_1 = tf.keras.layers.Conv2D(filters=out_dim * 2, kernel_size=1, strides=(1, 1), name=\"v_to_h_{}\".format(i))(v_stack)\n",
    "    h_stack_out = tf.keras.layers.Lambda(lambda inputs: gate(inputs), name=\"h_gate_{}\".format(i))(h_stack + h_stack_1)\n",
    "\n",
    "    h_stack_out = tf.keras.layers.Conv2D(filters=out_dim, kernel_size=1, strides=(1, 1), name=\"res_conv_{}\".format(i))(\n",
    "        h_stack_out)\n",
    "    if residual:\n",
    "        h_stack_out += h_stack_in\n",
    "    return v_stack_out, h_stack_out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### PixelCNN Prior Network ###########################################################################################\n",
    "\n",
    "pixelcnn_prior_inputs = tf.keras.layers.Input(shape=(SIZE, SIZE), name='pixelcnn_prior_inputs', dtype=tf.int32)\n",
    "z_q = model.vq_vae.quantize_encoding(pixelcnn_prior_inputs)  # maps indices to the actual codebook\n",
    "\n",
    "v_stack_in, h_stack_in = z_q, z_q\n",
    "for i in range(PIXELCNN_NUM_BLOCKS):\n",
    "    mask = 'b' if i > 0 else 'a'\n",
    "    kernel_size = 3 if i > 0 else 7\n",
    "    residual = True if i > 0 else False\n",
    "    v_stack_in, h_stack_in = gated_masked_conv2d(v_stack_in, h_stack_in, PIXELCNN_NUM_FEATURE_MAPS,\n",
    "                                                 kernel=kernel_size, residual=residual, i=i + 1)\n",
    "\n",
    "fc1 = tf.keras.layers.Conv2D(filters=PIXELCNN_NUM_FEATURE_MAPS, kernel_size=1, name=\"fc1\")(h_stack_in)\n",
    "fc2 = tf.keras.layers.Conv2D(filters=NUM_LATENT_K, kernel_size=1, name=\"fc2\")(fc1)\n",
    "# outputs logits for probabilities of codebook indices for each cell\n",
    "\n",
    "pixelcnn_prior = tf.keras.Model(inputs=pixelcnn_prior_inputs, outputs=fc2, name='pixelcnn-prior')\n",
    "\n",
    "# Distribution to sample from the pixelcnn\n",
    "samples = tfp.distributions.Categorical(logits=fc2).sample()\n",
    "prior_sampler = tf.keras.Model(inputs=pixelcnn_prior_inputs, outputs=samples, name='pixelcnn-prior-sampler')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Train PixelCNN Prior Network #####################################################################################\n",
    "\n",
    "pixelcnn_prior.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       metrics=tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "                       optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "\n",
    "prior_history = pixelcnn_prior.fit(z_train, z_train,\n",
    "                                   epochs=PIXELCNN_NUM_EPOCHS,\n",
    "                                   batch_size=PIXELCNN_BATCH_SIZE,\n",
    "                                   verbose=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Loss and Accuracy Graphs #####################################################################################\n",
    "\n",
    "fig=plt.figure(figsize=(30, 10))\n",
    "\n",
    "x = np.arange(0, len(prior_history.history['loss']))\n",
    "y1 = prior_history.history['loss']\n",
    "y2 = prior_history.history['sparse_categorical_accuracy']\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x,y1)\n",
    "ax.set_title('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x,y2)\n",
    "ax.set_title('Sparse Categorical Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('sparse_categorical_accuracy')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Generate Samples #################################################################################################\n",
    "\n",
    "fig = plt.figure(figsize=(40, 30))\n",
    "id=42\n",
    "\n",
    "for i in tqdm.tqdm(range(12)):\n",
    "    out = prior_sampler(z_train[id,...])\n",
    "    out = model.vq_vae.quantize_encoding(out)\n",
    "    X = model.decoder(out).numpy()\n",
    "    X = (X - X.min()) / (X.max() - X.min()) * 255\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i + 1)\n",
    "    ax.imshow(np.squeeze(X), cmap='gray')\n",
    "    ax.set_title(f'Sample {i}')\n",
    "\n",
    "    # cv2.imwrite(os.path.join(SAVE_DIR, f'code_{id}_sample_{i}_gen.jpg'), X[0,...]*255)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}